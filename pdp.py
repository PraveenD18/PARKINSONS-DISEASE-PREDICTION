# -*- coding: utf-8 -*-
"""PDP

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tdhlDQY_-UDZz2-oy_Y0yTaxMu9HuvMA
"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns

parkinsons_data = pd.read_csv('/content/drive/MyDrive/Parkinsson disease.csv')
parkinsons_data

parkinsons_data.isnull().sum()

fig, (axis1, axis2) = plt.subplots(1,2,figsize=(14,8))

sns.violinplot(x="status", y="RPDE", data=parkinsons_data, ax=axis1, palette="Set2")
axis1.set_title("status vs RPDE")

sns.violinplot(x="status", y="DFA", data=parkinsons_data, ax=axis2, palette="Set3")
axis2.set_title("status vs DFA")

fig, (axis1, axis2) = plt.subplots(1,2,figsize=(14,8))

sns.boxplot(x="status", y="NHR", data=parkinsons_data, ax=axis1)
axis1.set_title("status vs NHR")

sns.boxplot(x="status", y="HNR", data=parkinsons_data, ax=axis2, palette="Set2")
axis2.set_title("status vs HNR")

"""# **Steps for building a preparing a Model in Machine Learning**
Step 1: Data preprocessing

        * split the data to features and labels
        * Scale the features to same scale
        * Spli the data into training and testing portion
"""

labels  = parkinsons_data['status']
features = parkinsons_data.drop(['status','name'],axis=1)
print(labels)

print(features.shape)

features.boxplot(figsize=(15,7))

# checking with PCA if we get same values
from sklearn import decomposition
data = np.array(features)
print(pd.DataFrame(data))

PCA = decomposition.PCA(n_components=2)
data_pca = PCA.fit_transform(data)
print(data_pca)

data_pca_n = PCA.inverse_transform(data_pca)
print(pd.DataFrame(data_pca_n))

"""Since, after applying PCA we get the same dimensions so all the features are important"""

from sklearn.model_selection import StratifiedShuffleSplit
split=StratifiedShuffleSplit(n_splits=3,test_size=0.2,random_state=24)

for train_index,test_index in split.split(features,labels):
    strat_train_set_x,strat_train_y=features.loc[train_index],labels.loc[train_index]
    strat_test_set_x,strat_test_y=features.loc[test_index],labels.loc[test_index] #completely unseen to our classifier

print(strat_train_set_x.shape,strat_train_y.shape)
print(strat_test_set_x.shape,strat_test_y.shape)

#Scale the data
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import RobustScaler

num_pl=Pipeline([('rs',RobustScaler())])

strat_train_set_x=num_pl.fit_transform(strat_train_set_x)
strat_test_set_x=num_pl.transform(strat_test_set_x)

"""Step 2: Applying Classification Algorithms"""

#KNN Model
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(algorithm='auto',leaf_size=30,metric='minkowski',metric_params=None,
                           n_jobs=None, n_neighbors=5,p=2,weights='uniform')
knn.fit(strat_train_set_x, strat_train_y)

print("KNN with k=5 got {} % accuracy on the test set.".format(accuracy_score(strat_test_y, knn.predict(strat_test_set_x))*100))

print("KNN with k=5 got {} % accuracy on the train set.".format(accuracy_score(strat_train_y, knn.predict(strat_train_set_x))*100))

# KNN Grid Search
params_dict = {'n_neighbors':[3, 5, 9, 15], 'p':[1, 2, 3], 'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute']}
gs = GridSearchCV(knn, param_grid=params_dict, verbose=10, cv=10)

gs.fit(strat_train_set_x, strat_train_y)

print(gs.best_estimator_)

new_knn = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=3, p=1,
           weights='uniform')
new_knn.fit(strat_train_set_x, strat_train_y)

knn_pred = new_knn.predict(strat_test_set_x)
print(confusion_matrix(knn_pred, strat_test_y))

print("KNN - fine tuned, got {} % accuracy on the test set.".format(accuracy_score(strat_test_y, new_knn.predict(strat_test_set_x))*100))

from sklearn.linear_model import LogisticRegression

# Logistic Regression

parameters = {'penalty': ['l1', 'l2'],
              'C': [0.1, 0.4, 0.8, 1, 2, 5,10,20,30]}

grid_search=GridSearchCV(estimator = LogisticRegression() ,param_grid=parameters,cv=10,n_jobs=-1,verbose=2)
grid_search.fit(strat_train_set_x, strat_train_y)

log_reg = grid_search.best_estimator_

grid_search.best_params_

y_pred=log_reg.predict(strat_test_set_x)

print("\n",confusion_matrix(strat_test_y, y_pred))
log_reg_acc = accuracy_score(strat_test_y,y_pred)

print("\nAccuracy Score {}".format(log_reg_acc))
print("Classification report: \n{}".format(classification_report(strat_test_y,y_pred)))

# Decision Tree Classifier
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier()
dt.fit(strat_train_set_x, strat_train_y)

dt_pred = dt.predict(strat_test_set_x)
print(confusion_matrix(strat_test_y, dt_pred))

print("Decision Tree Classifier - default, got {}% accuracy on the test set.".format(accuracy_score(strat_test_y, dt.predict(strat_test_set_x))*100))

# Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(n_estimators=350)
rfc.fit(strat_train_set_x, strat_train_y)

rfc_pred = rfc.predict(strat_test_set_x)
print(confusion_matrix(strat_test_y, rfc_pred))

print("Random Forest Classifier - default, got {}% accuracy on the test set.".format(accuracy_score(strat_test_y, rfc.predict(strat_test_set_x))*100))

# support vector classifier

from sklearn.svm import LinearSVC

svc = LinearSVC()

parameters = {
      'penalty':['l1', 'l2'],
      'max_iter': [10,20,50,100,1000],
      'C': [0.1, 0.4, 0.8, 1, 2, 5,10,20,30],
              }

grid_search=GridSearchCV(estimator=svc ,param_grid=parameters,cv=10,n_jobs=-1,verbose=2)
grid_search.fit(strat_train_set_x,strat_train_y)

svc = grid_search.best_estimator_

grid_search.best_params_

svc_pred=svc.predict(strat_test_set_x)

print("\n",confusion_matrix(strat_test_y,svc_pred))
svc_acc = accuracy_score(strat_test_y,svc_pred)
print("\nAccuracy Score {}".format(svc_acc))
print("Classification report: \n{}".format(classification_report(strat_test_y,svc_pred)))

# Extreme Gradient Boosting
from xgboost import XGBClassifier
xgb = XGBClassifier()

parameters = {'min_child_weight' : np.arange(0,20),
              'max_depth': [2, 4, 5, 7, 9, 10]}

grid_search=GridSearchCV(estimator=xgb ,param_grid=parameters,cv=10,n_jobs=-1,verbose=2)
grid_search.fit(strat_train_set_x, strat_train_y)

xgb = grid_search.best_estimator_

grid_search.best_params_

y_pred=xgb.predict(strat_test_set_x)

print("\n",confusion_matrix(strat_test_y,y_pred))
xgb_acc = accuracy_score(strat_test_y,y_pred)
print("\nAccuracy Score {}".format(xgb_acc))
print("Classification report: \n{}".format(classification_report(strat_test_y,y_pred)))

# Stacking Ensemble Classifier

from sklearn.ensemble import StackingClassifier
estimators = [ ('xgb', xgb ),
              ('svc',svc ),
              ('log_Reg', log_reg),
              ('rfc', rfc),
              ('dt', dt),
              ('new_knn',new_knn)]


stack = StackingClassifier(estimators=estimators ,final_estimator= svc)

stack.fit(strat_train_set_x, strat_train_y)
stack_predicted = stack.predict(strat_test_set_x)

stack_conf_matrix = confusion_matrix(strat_test_y, stack_predicted)
stack_acc_score = accuracy_score(strat_test_y, stack_predicted)

print("confussion matrix")
print(stack_conf_matrix)
print("\n")
print("Accuracy of Stacking Classifier:",stack_acc_score*100,'\n')
print(classification_report(strat_test_y,stack_predicted))

!pip install catboost

#Cat Boost Classifier

from catboost import CatBoostClassifier

clf = CatBoostClassifier(
    iterations=5,
    learning_rate=0.1,
    #loss_function='CrossEntropy'
)


clf.fit(strat_train_set_x,strat_train_y,
        eval_set=(strat_test_set_x, strat_test_y),
        verbose=False
)

print('CatBoost model is fitted: ' + str(clf.is_fitted()))
print('CatBoost model parameters:')
print(clf.get_params())

clf_predicted = clf.predict(strat_test_set_x)
clf_conf_matrix = confusion_matrix(strat_test_y, clf_predicted)
clf_acc_score = accuracy_score(strat_test_y, clf_predicted)

print("confussion matrix")
print(clf_conf_matrix)
print("\n")
print("Accuracy of Stacking Classifier:",clf_acc_score*100,'\n')
print(classification_report(strat_test_y,clf_predicted))

"""#**Torch**

----


"""

!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split

class park_model(nn.Module):
    # We have hidden layer containg 30 neurons and another containg 20 neurons
    # We will use only 22 feature from the dataset we will exclude name and status
    def __init__(self,in_features=22, h1=20, h2=30, out_features=1):
        super().__init__()
        self.fc1 = nn.Linear(in_features, h1)
        self.fc2 = nn.Linear(h1, h2)
        self.out = nn.Linear(h2, out_features)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.out(x)
        return x

torch.manual_seed(7)
model = park_model()

model

y  = parkinsons_data['status'].values
X = parkinsons_data.drop(['status','name'],axis=1).values

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=8)

y_train = y_train.reshape((X_train.shape[0],1))

X_train = torch.FloatTensor(X_train)
X_test = torch.FloatTensor(X_test)

y_train = torch.FloatTensor(y_train)
y_test = torch.FloatTensor(y_test)

print(X_train.shape)
print(y_train.shape)

criterion = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)

epochs = 1000
losses = []

for i in range(epochs):
    i+=1
    y_pred = model.forward(X_train)
    loss = criterion(y_pred, y_train)
    losses.append(loss)

    # a neat trick to save screen space:
    if i%10 == 1:
        print(f'epoch: {i:2}  loss: {loss.item():10.8f}')

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

with torch.no_grad():
    y_val = model.forward(X_test)
    loss = criterion(y_val, y_test)
print(f'{loss:.8f}')

import tensorflow as tf
import keras

modeltf = tf.keras.models.Sequential([
    tf.keras.layers.Dense(20),
    tf.keras.layers.Dense(30),
# tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

modeltf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

modeltf.fit(X_train, y_train, epochs=100)